{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get text data, Shakespearean sonnet\n",
    "with open(r'C:\\Users\\dearm\\OneDrive\\Documents\\PYTORCH_NOTEBOOKS\\Data\\shakespeare.txt','r',encoding='utf8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But as the riper should by time decease,\n",
      "  His tender heir might bear his memory:\n",
      "  But thou contracted to thine own bright eyes,\n",
      "  Feed'st thy light's flame with self-substantial fuel,\n",
      "  Making a famine where abundance lies,\n",
      "  Thy self thy foe, to thy sweet self too cruel:\n",
      "  Thou that art now the world's fresh ornament,\n",
      "  And only herald to the gaudy spring,\n",
      "  Within thine own bud buriest thy content,\n",
      "  And tender churl mak'st waste in niggarding:\n",
      "    Pity the world, or else this glutton be,\n",
      "    To eat the world's due, by the grave and thee.\n",
      "\n",
      "\n",
      "                     2\n",
      "  When forty winters shall besiege thy brow,\n",
      "  And dig deep trenches in thy beauty's field,\n",
      "  Thy youth's proud livery so gazed on now,\n",
      "  Will be a tattered weed of small worth held:  \n",
      "  Then being asked, where all thy beauty lies,\n",
      "  Where all the treasure of thy lusty days;\n",
      "  To say within thine own deep su\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the entire text\n",
    "all_characters = set(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numbers -> letters\n",
    "decoder = dict(enumerate(all_characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([(0, 'b'), (1, 'p'), (2, 'u'), (3, 'f'), (4, 'm'), (5, ' '), (6, ':'), (7, 'r'), (8, 'Y'), (9, '('), (10, 'a'), (11, '1'), (12, 'X'), (13, 'n'), (14, 'B'), (15, 'J'), (16, '3'), (17, '5'), (18, 'C'), (19, 's'), (20, 'w'), (21, '2'), (22, 'Q'), (23, 'R'), (24, 'd'), (25, 'E'), (26, '}'), (27, '0'), (28, 'A'), (29, '9'), (30, 'h'), (31, '&'), (32, 'T'), (33, 'F'), (34, '7'), (35, 'y'), (36, '`'), (37, 'v'), (38, 'N'), (39, '?'), (40, 'g'), (41, '['), (42, '_'), (43, 'k'), (44, 't'), (45, '-'), (46, 'H'), (47, ','), (48, '8'), (49, 'P'), (50, '<'), (51, '\"'), (52, '>'), (53, 'W'), (54, ';'), (55, '|'), (56, 'e'), (57, 'D'), (58, 'G'), (59, 'L'), (60, 'c'), (61, 'O'), (62, '4'), (63, 'o'), (64, 'M'), (65, 'i'), (66, \"'\"), (67, 'V'), (68, '6'), (69, '!'), (70, ']'), (71, 'U'), (72, 'j'), (73, '.'), (74, 'x'), (75, ')'), (76, 'I'), (77, 'l'), (78, 'S'), (79, 'Z'), (80, 'z'), (81, 'q'), (82, '\\n'), (83, 'K')])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decoder\n",
    "decoder.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# letters -> numbers\n",
    "encoder = {char: ind for ind,char in decoder.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = np.array([encoder[char] for char in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([82,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
       "        5,  5,  5,  5,  5, 11, 82,  5,  5, 33,  7, 63,  4,  5,  3, 10, 65,\n",
       "        7, 56, 19, 44,  5, 60,  7, 56, 10, 44,  2,  7, 56, 19,  5, 20, 56,\n",
       "        5, 24, 56, 19, 65,  7, 56,  5, 65, 13, 60,  7, 56, 10, 19, 56, 47,\n",
       "       82,  5,  5, 32, 30, 10, 44,  5, 44, 30, 56,  7, 56,  0, 35,  5,  0,\n",
       "       56, 10,  2, 44, 35, 66, 19,  5,  7, 63, 19, 56,  5,  4, 65, 40, 30,\n",
       "       44,  5, 13, 56, 37, 56,  7,  5, 24, 65, 56, 47, 82,  5,  5, 14,  2,\n",
       "       44,  5, 10, 19,  5, 44, 30, 56,  5,  7, 65,  1, 56,  7,  5, 19, 30,\n",
       "       63,  2, 77, 24,  5,  0, 35,  5, 44, 65,  4, 56,  5, 24, 56, 60, 56,\n",
       "       10, 19, 56, 47, 82,  5,  5, 46, 65, 19,  5, 44, 56, 13, 24, 56,  7,\n",
       "        5, 30, 56, 65,  7,  5,  4, 65, 40, 30, 44,  5,  0, 56, 10,  7,  5,\n",
       "       30, 65, 19,  5,  4, 56,  4, 63,  7, 35,  6, 82,  5,  5, 14,  2, 44,\n",
       "        5, 44, 30, 63,  2,  5, 60, 63, 13, 44,  7, 10, 60, 44, 56, 24,  5,\n",
       "       44, 63,  5, 44, 30, 65, 13, 56,  5, 63, 20, 13,  5,  0,  7, 65, 40,\n",
       "       30, 44,  5, 56, 35, 56, 19, 47, 82,  5,  5, 33, 56, 56, 24, 66, 19,\n",
       "       44,  5, 44, 30, 35,  5, 77, 65, 40, 30, 44, 66, 19,  5,  3, 77, 10,\n",
       "        4, 56,  5, 20, 65, 44, 30,  5, 19, 56, 77,  3, 45, 19,  2,  0, 19,\n",
       "       44, 10, 13, 44, 65, 10, 77,  5,  3,  2, 56, 77, 47, 82,  5,  5, 64,\n",
       "       10, 43, 65, 13, 40,  5, 10,  5,  3, 10,  4, 65, 13, 56,  5, 20, 30,\n",
       "       56,  7, 56,  5, 10,  0,  2, 13, 24, 10, 13, 60, 56,  5, 77, 65, 56,\n",
       "       19, 47, 82,  5,  5, 32, 30, 35,  5, 19, 56, 77,  3,  5, 44, 30, 35,\n",
       "        5,  3, 63, 56, 47,  5, 44, 63,  5, 44, 30, 35,  5, 19, 20, 56, 56,\n",
       "       44,  5, 19, 56, 77,  3,  5, 44, 63, 63,  5, 60,  7,  2, 56, 77,  6,\n",
       "       82,  5,  5, 32, 30, 63,  2,  5, 44, 30, 10, 44,  5, 10,  7, 44,  5,\n",
       "       13, 63, 20,  5, 44, 30, 56,  5, 20, 63,  7, 77, 24, 66, 19,  5,  3,\n",
       "        7, 56, 19, 30,  5, 63,  7, 13, 10,  4, 56, 13, 44, 47, 82,  5,  5,\n",
       "       28, 13, 24,  5, 63, 13, 77, 35,  5, 30, 56,  7, 10, 77, 24,  5, 44,\n",
       "       63,  5, 44, 30, 56,  5, 40, 10,  2, 24, 35,  5, 19,  1,  7, 65, 13,\n",
       "       40, 47, 82,  5,  5, 53, 65, 44, 30, 65, 13,  5, 44, 30, 65, 13, 56,\n",
       "        5, 63, 20, 13,  5,  0,  2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding\n",
    "def one_hot_encoder(encoded_text, num_uni_chars):\n",
    "    '''\n",
    "    encoded_text : batch of encoded text\n",
    "    \n",
    "    num_uni_chars = number of unique characters (len(set(text)))\n",
    "    '''\n",
    "    \n",
    "    # METHOD FROM:\n",
    "    # https://stackoverflow.com/questions/29831489/convert-encoded_textay-of-indices-to-1-hot-encoded-numpy-encoded_textay\n",
    "      \n",
    "    # Create a placeholder for zeros.\n",
    "    one_hot = np.zeros((encoded_text.size, num_uni_chars))\n",
    "    \n",
    "    # Convert data type for later use with pytorch (errors if we dont!)\n",
    "    one_hot = one_hot.astype(np.float32)\n",
    "\n",
    "    # Using fancy indexing fill in the 1s at the correct index locations\n",
    "    one_hot[np.arange(one_hot.shape[0]), encoded_text.flatten()] = 1.0\n",
    "    \n",
    "\n",
    "    # Reshape it so it matches the batch sahe\n",
    "    one_hot = one_hot.reshape((*encoded_text.shape, num_uni_chars))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoder(np.array([1,2,0]),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating training batches, batches of characters where next character in sequence is the label\n",
    "def generate_batches(encoded_text, samp_per_batch=10, seq_len=50):\n",
    "    \n",
    "    '''\n",
    "    Generate (using yield) batches for training.\n",
    "    \n",
    "    X: Encoded Text of length seq_len\n",
    "    Y: Encoded Text shifted by one\n",
    "    \n",
    "    Example:\n",
    "    \n",
    "    X:\n",
    "    \n",
    "    [[1 2 3]]\n",
    "    \n",
    "    Y:\n",
    "    \n",
    "    [[ 2 3 4]]\n",
    "    \n",
    "    encoded_text : Complete Encoded Text to make batches from\n",
    "    batch_size : Number of samples per batch\n",
    "    seq_len : Length of character sequence\n",
    "       \n",
    "    '''\n",
    "    \n",
    "    # Total number of characters per batch\n",
    "    # Example: If samp_per_batch is 2 and seq_len is 50, then 100\n",
    "    # characters come out per batch\n",
    "    char_per_batch = samp_per_batch * seq_len\n",
    "    \n",
    "    \n",
    "    # Number of batches available to make\n",
    "    # Use int() to roun to nearest integer\n",
    "    num_batches_avail = int(len(encoded_text)/char_per_batch)\n",
    "    \n",
    "    # Cut off end of encoded_text that\n",
    "    # won't fit evenly into a batch\n",
    "    encoded_text = encoded_text[:num_batches_avail * char_per_batch]\n",
    "    \n",
    "    \n",
    "    # Reshape text into rows the size of a batch\n",
    "    encoded_text = encoded_text.reshape((samp_per_batch, -1))\n",
    "    \n",
    "\n",
    "    # Go through each row in array\n",
    "    for n in range(0, encoded_text.shape[1], seq_len):\n",
    "        \n",
    "        # Grab feature characters\n",
    "        x = encoded_text[:, n:n+seq_len]\n",
    "        \n",
    "        # y is the target shifted over by 1\n",
    "        y = np.zeros_like(x)\n",
    "       \n",
    "        #\n",
    "        try:\n",
    "            y[:, :-1] = x[:, 1:]\n",
    "            y[:, -1]  = encoded_text[:, n+seq_len]\n",
    "            \n",
    "        # FOR POTENTIAL INDEXING ERROR AT THE END    \n",
    "        except:\n",
    "            y[:, :-1] = x[:, 1:]\n",
    "            y[:, -1] = encoded_text[:, 0]\n",
    "            \n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPU check, would take a lot longer on CPU\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "class CharModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, all_chars, num_hidden=256, num_layers=4,drop_prob=0.5,use_gpu=False):\n",
    "        \n",
    "        \n",
    "        # SET UP ATTRIBUTES\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.num_layers = num_layers\n",
    "        self.num_hidden = num_hidden\n",
    "        self.use_gpu = use_gpu\n",
    "        \n",
    "        #CHARACTER SET, ENCODER, and DECODER\n",
    "        self.all_chars = all_chars\n",
    "        self.decoder = dict(enumerate(all_chars))\n",
    "        self.encoder = {char: ind for ind,char in decoder.items()}\n",
    "        \n",
    "        \n",
    "        self.lstm = nn.LSTM(len(self.all_chars), num_hidden, num_layers, dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        self.fc_linear = nn.Linear(num_hidden, len(self.all_chars))\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "                  \n",
    "        \n",
    "        lstm_output, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        \n",
    "        drop_output = self.dropout(lstm_output)\n",
    "        \n",
    "        drop_output = drop_output.contiguous().view(-1, self.num_hidden)\n",
    "        \n",
    "        \n",
    "        final_out = self.fc_linear(drop_output)\n",
    "        \n",
    "        \n",
    "        return final_out, hidden\n",
    "    \n",
    "    \n",
    "    def hidden_state(self, batch_size):\n",
    "        '''\n",
    "        Used as separate method to account for both GPU and CPU users\n",
    "        '''\n",
    "        \n",
    "        if self.use_gpu:\n",
    "            \n",
    "            hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda(),\n",
    "                     torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda())\n",
    "        else:\n",
    "            hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden),\n",
    "                     torch.zeros(self.num_layers,batch_size,self.num_hidden))\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model\n",
    "model = CharModel(\n",
    "    all_chars=all_characters,\n",
    "    num_hidden=512,\n",
    "    num_layers=3,\n",
    "    drop_prob=0.5,\n",
    "    use_gpu=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of parameters should be same magnitude as number of characters in text\n",
    "total_param  = []\n",
    "for p in model.parameters():\n",
    "    total_param.append(int(p.numel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5470292"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(total_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5445609"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss and optimizer functions\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of data to be used for training\n",
    "train_percent = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find cutoff index for train set\n",
    "train_ind = int(len(encoded_text) * (train_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set train and validation sets\n",
    "train_data = encoded_text[:train_ind]\n",
    "val_data = encoded_text[train_ind:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set training variables\n",
    "epochs = 30\n",
    "# batch size \n",
    "batch_size = 100\n",
    "\n",
    "# length of sequence\n",
    "seq_len = 100\n",
    "\n",
    "# for printing report purposes\n",
    "# always start at 0\n",
    "tracker = 0\n",
    "\n",
    "# number of characters in text\n",
    "num_char = max(encoded_text)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Step: 25 Val Loss: 3.2057440280914307\n",
      "Epoch: 0 Step: 50 Val Loss: 3.1939539909362793\n",
      "Epoch: 0 Step: 75 Val Loss: 3.197103500366211\n",
      "Epoch: 0 Step: 100 Val Loss: 3.1606411933898926\n",
      "Epoch: 0 Step: 125 Val Loss: 3.043724536895752\n",
      "Epoch: 0 Step: 150 Val Loss: 2.9737250804901123\n",
      "Epoch: 0 Step: 175 Val Loss: 2.8906137943267822\n",
      "Epoch: 0 Step: 200 Val Loss: 2.754720449447632\n",
      "Epoch: 0 Step: 225 Val Loss: 2.680861473083496\n",
      "Epoch: 0 Step: 250 Val Loss: 2.598978042602539\n",
      "Epoch: 0 Step: 275 Val Loss: 2.474199056625366\n",
      "Epoch: 0 Step: 300 Val Loss: 2.3618345260620117\n",
      "Epoch: 0 Step: 325 Val Loss: 2.281284809112549\n",
      "Epoch: 0 Step: 350 Val Loss: 2.228391647338867\n",
      "Epoch: 0 Step: 375 Val Loss: 2.1794230937957764\n",
      "Epoch: 0 Step: 400 Val Loss: 2.1438920497894287\n",
      "Epoch: 0 Step: 425 Val Loss: 2.100783109664917\n",
      "Epoch: 0 Step: 450 Val Loss: 2.075124979019165\n",
      "Epoch: 0 Step: 475 Val Loss: 2.033742904663086\n",
      "Epoch: 1 Step: 500 Val Loss: 2.0101773738861084\n",
      "Epoch: 1 Step: 525 Val Loss: 1.9862054586410522\n",
      "Epoch: 1 Step: 550 Val Loss: 1.9698326587677002\n",
      "Epoch: 1 Step: 575 Val Loss: 1.9447872638702393\n",
      "Epoch: 1 Step: 600 Val Loss: 1.926524043083191\n",
      "Epoch: 1 Step: 625 Val Loss: 1.902355670928955\n",
      "Epoch: 1 Step: 650 Val Loss: 1.8765153884887695\n",
      "Epoch: 1 Step: 675 Val Loss: 1.8717814683914185\n",
      "Epoch: 1 Step: 700 Val Loss: 1.8445203304290771\n",
      "Epoch: 1 Step: 725 Val Loss: 1.8228970766067505\n",
      "Epoch: 1 Step: 750 Val Loss: 1.8088335990905762\n",
      "Epoch: 1 Step: 775 Val Loss: 1.7979847192764282\n",
      "Epoch: 1 Step: 800 Val Loss: 1.7890355587005615\n",
      "Epoch: 1 Step: 825 Val Loss: 1.7671197652816772\n",
      "Epoch: 1 Step: 850 Val Loss: 1.7550599575042725\n",
      "Epoch: 1 Step: 875 Val Loss: 1.7443000078201294\n",
      "Epoch: 1 Step: 900 Val Loss: 1.7338474988937378\n",
      "Epoch: 1 Step: 925 Val Loss: 1.717467188835144\n",
      "Epoch: 1 Step: 950 Val Loss: 1.7083418369293213\n",
      "Epoch: 1 Step: 975 Val Loss: 1.6938923597335815\n",
      "Epoch: 2 Step: 1000 Val Loss: 1.6873892545700073\n",
      "Epoch: 2 Step: 1025 Val Loss: 1.679936170578003\n",
      "Epoch: 2 Step: 1050 Val Loss: 1.6697226762771606\n",
      "Epoch: 2 Step: 1075 Val Loss: 1.665933609008789\n",
      "Epoch: 2 Step: 1100 Val Loss: 1.6570876836776733\n",
      "Epoch: 2 Step: 1125 Val Loss: 1.645119547843933\n",
      "Epoch: 2 Step: 1150 Val Loss: 1.6335110664367676\n",
      "Epoch: 2 Step: 1175 Val Loss: 1.6293083429336548\n",
      "Epoch: 2 Step: 1200 Val Loss: 1.6173573732376099\n",
      "Epoch: 2 Step: 1225 Val Loss: 1.6132380962371826\n",
      "Epoch: 2 Step: 1250 Val Loss: 1.6062476634979248\n",
      "Epoch: 2 Step: 1275 Val Loss: 1.5955215692520142\n",
      "Epoch: 2 Step: 1300 Val Loss: 1.592715859413147\n",
      "Epoch: 2 Step: 1325 Val Loss: 1.5892939567565918\n",
      "Epoch: 2 Step: 1350 Val Loss: 1.589297890663147\n",
      "Epoch: 2 Step: 1375 Val Loss: 1.577082872390747\n",
      "Epoch: 2 Step: 1400 Val Loss: 1.5703794956207275\n",
      "Epoch: 2 Step: 1425 Val Loss: 1.5645182132720947\n",
      "Epoch: 2 Step: 1450 Val Loss: 1.5574597120285034\n",
      "Epoch: 3 Step: 1475 Val Loss: 1.556631088256836\n",
      "Epoch: 3 Step: 1500 Val Loss: 1.5525271892547607\n",
      "Epoch: 3 Step: 1525 Val Loss: 1.5422072410583496\n",
      "Epoch: 3 Step: 1550 Val Loss: 1.5403409004211426\n",
      "Epoch: 3 Step: 1575 Val Loss: 1.5375187397003174\n",
      "Epoch: 3 Step: 1600 Val Loss: 1.5310171842575073\n",
      "Epoch: 3 Step: 1625 Val Loss: 1.5250647068023682\n",
      "Epoch: 3 Step: 1650 Val Loss: 1.5228310823440552\n",
      "Epoch: 3 Step: 1675 Val Loss: 1.5132378339767456\n",
      "Epoch: 3 Step: 1700 Val Loss: 1.505979299545288\n",
      "Epoch: 3 Step: 1725 Val Loss: 1.50400972366333\n",
      "Epoch: 3 Step: 1750 Val Loss: 1.5038751363754272\n",
      "Epoch: 3 Step: 1775 Val Loss: 1.5016165971755981\n",
      "Epoch: 3 Step: 1800 Val Loss: 1.497180461883545\n",
      "Epoch: 3 Step: 1825 Val Loss: 1.4957700967788696\n",
      "Epoch: 3 Step: 1850 Val Loss: 1.4990710020065308\n",
      "Epoch: 3 Step: 1875 Val Loss: 1.4894548654556274\n",
      "Epoch: 3 Step: 1900 Val Loss: 1.4842482805252075\n",
      "Epoch: 3 Step: 1925 Val Loss: 1.480559229850769\n",
      "Epoch: 3 Step: 1950 Val Loss: 1.4732089042663574\n",
      "Epoch: 4 Step: 1975 Val Loss: 1.4747180938720703\n",
      "Epoch: 4 Step: 2000 Val Loss: 1.4754738807678223\n",
      "Epoch: 4 Step: 2025 Val Loss: 1.4699313640594482\n",
      "Epoch: 4 Step: 2050 Val Loss: 1.4620412588119507\n",
      "Epoch: 4 Step: 2075 Val Loss: 1.4639638662338257\n",
      "Epoch: 4 Step: 2100 Val Loss: 1.460836410522461\n",
      "Epoch: 4 Step: 2125 Val Loss: 1.4572629928588867\n",
      "Epoch: 4 Step: 2150 Val Loss: 1.4534348249435425\n",
      "Epoch: 4 Step: 2175 Val Loss: 1.4500232934951782\n",
      "Epoch: 4 Step: 2200 Val Loss: 1.4517923593521118\n",
      "Epoch: 4 Step: 2225 Val Loss: 1.4420853853225708\n",
      "Epoch: 4 Step: 2250 Val Loss: 1.4437259435653687\n",
      "Epoch: 4 Step: 2275 Val Loss: 1.445865273475647\n",
      "Epoch: 4 Step: 2300 Val Loss: 1.442816138267517\n",
      "Epoch: 4 Step: 2325 Val Loss: 1.4441877603530884\n",
      "Epoch: 4 Step: 2350 Val Loss: 1.4422788619995117\n",
      "Epoch: 4 Step: 2375 Val Loss: 1.4401025772094727\n",
      "Epoch: 4 Step: 2400 Val Loss: 1.444421410560608\n",
      "Epoch: 4 Step: 2425 Val Loss: 1.4342416524887085\n",
      "Epoch: 4 Step: 2450 Val Loss: 1.4322155714035034\n",
      "Epoch: 5 Step: 2475 Val Loss: 1.4287770986557007\n",
      "Epoch: 5 Step: 2500 Val Loss: 1.4300583600997925\n",
      "Epoch: 5 Step: 2525 Val Loss: 1.4240821599960327\n",
      "Epoch: 5 Step: 2550 Val Loss: 1.4303407669067383\n",
      "Epoch: 5 Step: 2575 Val Loss: 1.4195746183395386\n",
      "Epoch: 5 Step: 2600 Val Loss: 1.4217274188995361\n",
      "Epoch: 5 Step: 2625 Val Loss: 1.4207876920700073\n",
      "Epoch: 5 Step: 2650 Val Loss: 1.4114454984664917\n",
      "Epoch: 5 Step: 2675 Val Loss: 1.404885172843933\n",
      "Epoch: 5 Step: 2700 Val Loss: 1.4091932773590088\n",
      "Epoch: 5 Step: 2725 Val Loss: 1.4175891876220703\n",
      "Epoch: 5 Step: 2750 Val Loss: 1.4126477241516113\n",
      "Epoch: 5 Step: 2775 Val Loss: 1.4075443744659424\n",
      "Epoch: 5 Step: 2800 Val Loss: 1.4156279563903809\n",
      "Epoch: 5 Step: 2825 Val Loss: 1.4138991832733154\n",
      "Epoch: 5 Step: 2850 Val Loss: 1.4093648195266724\n",
      "Epoch: 5 Step: 2875 Val Loss: 1.4122838973999023\n",
      "Epoch: 5 Step: 2900 Val Loss: 1.404646635055542\n",
      "Epoch: 5 Step: 2925 Val Loss: 1.4005415439605713\n",
      "Epoch: 6 Step: 2950 Val Loss: 1.4000338315963745\n",
      "Epoch: 6 Step: 2975 Val Loss: 1.4048577547073364\n",
      "Epoch: 6 Step: 3000 Val Loss: 1.4002431631088257\n",
      "Epoch: 6 Step: 3025 Val Loss: 1.3948150873184204\n",
      "Epoch: 6 Step: 3050 Val Loss: 1.3929996490478516\n",
      "Epoch: 6 Step: 3075 Val Loss: 1.3952115774154663\n",
      "Epoch: 6 Step: 3100 Val Loss: 1.3947356939315796\n",
      "Epoch: 6 Step: 3125 Val Loss: 1.3901236057281494\n",
      "Epoch: 6 Step: 3150 Val Loss: 1.3909636735916138\n",
      "Epoch: 6 Step: 3175 Val Loss: 1.3862069845199585\n",
      "Epoch: 6 Step: 3200 Val Loss: 1.3867146968841553\n",
      "Epoch: 6 Step: 3225 Val Loss: 1.388553261756897\n",
      "Epoch: 6 Step: 3250 Val Loss: 1.3846930265426636\n",
      "Epoch: 6 Step: 3275 Val Loss: 1.379413366317749\n",
      "Epoch: 6 Step: 3300 Val Loss: 1.3879555463790894\n",
      "Epoch: 6 Step: 3325 Val Loss: 1.3876057863235474\n",
      "Epoch: 6 Step: 3350 Val Loss: 1.3865394592285156\n",
      "Epoch: 6 Step: 3375 Val Loss: 1.3843910694122314\n",
      "Epoch: 6 Step: 3400 Val Loss: 1.3848867416381836\n",
      "Epoch: 6 Step: 3425 Val Loss: 1.3786977529525757\n",
      "Epoch: 7 Step: 3450 Val Loss: 1.378352403640747\n",
      "Epoch: 7 Step: 3475 Val Loss: 1.3808605670928955\n",
      "Epoch: 7 Step: 3500 Val Loss: 1.3802869319915771\n",
      "Epoch: 7 Step: 3525 Val Loss: 1.3757400512695312\n",
      "Epoch: 7 Step: 3550 Val Loss: 1.3753994703292847\n",
      "Epoch: 7 Step: 3575 Val Loss: 1.3782615661621094\n",
      "Epoch: 7 Step: 3600 Val Loss: 1.3726835250854492\n",
      "Epoch: 7 Step: 3625 Val Loss: 1.3774161338806152\n",
      "Epoch: 7 Step: 3650 Val Loss: 1.3684141635894775\n",
      "Epoch: 7 Step: 3675 Val Loss: 1.3675353527069092\n",
      "Epoch: 7 Step: 3700 Val Loss: 1.365644931793213\n",
      "Epoch: 7 Step: 3725 Val Loss: 1.3667930364608765\n",
      "Epoch: 7 Step: 3750 Val Loss: 1.3656105995178223\n",
      "Epoch: 7 Step: 3775 Val Loss: 1.371000051498413\n",
      "Epoch: 7 Step: 3800 Val Loss: 1.3723037242889404\n",
      "Epoch: 7 Step: 3825 Val Loss: 1.3749979734420776\n",
      "Epoch: 7 Step: 3850 Val Loss: 1.3756953477859497\n",
      "Epoch: 7 Step: 3875 Val Loss: 1.3696105480194092\n",
      "Epoch: 7 Step: 3900 Val Loss: 1.3726667165756226\n",
      "Epoch: 8 Step: 3925 Val Loss: 1.372711420059204\n",
      "Epoch: 8 Step: 3950 Val Loss: 1.3687533140182495\n",
      "Epoch: 8 Step: 3975 Val Loss: 1.3661118745803833\n",
      "Epoch: 8 Step: 4000 Val Loss: 1.3608425855636597\n",
      "Epoch: 8 Step: 4025 Val Loss: 1.3658008575439453\n",
      "Epoch: 8 Step: 4050 Val Loss: 1.356764316558838\n",
      "Epoch: 8 Step: 4075 Val Loss: 1.3646247386932373\n",
      "Epoch: 8 Step: 4100 Val Loss: 1.3630502223968506\n",
      "Epoch: 8 Step: 4125 Val Loss: 1.3593865633010864\n",
      "Epoch: 8 Step: 4150 Val Loss: 1.352952480316162\n",
      "Epoch: 8 Step: 4175 Val Loss: 1.3534001111984253\n",
      "Epoch: 8 Step: 4200 Val Loss: 1.358030915260315\n",
      "Epoch: 8 Step: 4225 Val Loss: 1.3523774147033691\n",
      "Epoch: 8 Step: 4250 Val Loss: 1.3534396886825562\n",
      "Epoch: 8 Step: 4275 Val Loss: 1.3602627515792847\n",
      "Epoch: 8 Step: 4300 Val Loss: 1.3593579530715942\n",
      "Epoch: 8 Step: 4325 Val Loss: 1.3659236431121826\n",
      "Epoch: 8 Step: 4350 Val Loss: 1.3631173372268677\n",
      "Epoch: 8 Step: 4375 Val Loss: 1.3604730367660522\n",
      "Epoch: 8 Step: 4400 Val Loss: 1.359640121459961\n",
      "Epoch: 9 Step: 4425 Val Loss: 1.3598190546035767\n",
      "Epoch: 9 Step: 4450 Val Loss: 1.3573960065841675\n",
      "Epoch: 9 Step: 4475 Val Loss: 1.3559141159057617\n",
      "Epoch: 9 Step: 4500 Val Loss: 1.354369044303894\n",
      "Epoch: 9 Step: 4525 Val Loss: 1.3521558046340942\n",
      "Epoch: 9 Step: 4550 Val Loss: 1.3508917093276978\n",
      "Epoch: 9 Step: 4575 Val Loss: 1.3504717350006104\n",
      "Epoch: 9 Step: 4600 Val Loss: 1.3518810272216797\n",
      "Epoch: 9 Step: 4625 Val Loss: 1.346497893333435\n",
      "Epoch: 9 Step: 4650 Val Loss: 1.3453980684280396\n",
      "Epoch: 9 Step: 4675 Val Loss: 1.3486638069152832\n",
      "Epoch: 9 Step: 4700 Val Loss: 1.348639726638794\n",
      "Epoch: 9 Step: 4725 Val Loss: 1.346341609954834\n",
      "Epoch: 9 Step: 4750 Val Loss: 1.3471527099609375\n",
      "Epoch: 9 Step: 4775 Val Loss: 1.3493010997772217\n",
      "Epoch: 9 Step: 4800 Val Loss: 1.3526158332824707\n",
      "Epoch: 9 Step: 4825 Val Loss: 1.3505229949951172\n",
      "Epoch: 9 Step: 4850 Val Loss: 1.3537111282348633\n",
      "Epoch: 9 Step: 4875 Val Loss: 1.3532224893569946\n",
      "Epoch: 9 Step: 4900 Val Loss: 1.3527162075042725\n",
      "Epoch: 10 Step: 4925 Val Loss: 1.3499382734298706\n",
      "Epoch: 10 Step: 4950 Val Loss: 1.3522292375564575\n",
      "Epoch: 10 Step: 4975 Val Loss: 1.3466752767562866\n",
      "Epoch: 10 Step: 5000 Val Loss: 1.3448872566223145\n",
      "Epoch: 10 Step: 5025 Val Loss: 1.3400068283081055\n",
      "Epoch: 10 Step: 5050 Val Loss: 1.338464617729187\n",
      "Epoch: 10 Step: 5075 Val Loss: 1.3396331071853638\n",
      "Epoch: 10 Step: 5100 Val Loss: 1.3362915515899658\n",
      "Epoch: 10 Step: 5125 Val Loss: 1.332687258720398\n",
      "Epoch: 10 Step: 5150 Val Loss: 1.338239073753357\n",
      "Epoch: 10 Step: 5175 Val Loss: 1.3396852016448975\n",
      "Epoch: 10 Step: 5200 Val Loss: 1.3379766941070557\n",
      "Epoch: 10 Step: 5225 Val Loss: 1.3351458311080933\n",
      "Epoch: 10 Step: 5250 Val Loss: 1.343358039855957\n",
      "Epoch: 10 Step: 5275 Val Loss: 1.3414477109909058\n",
      "Epoch: 10 Step: 5300 Val Loss: 1.3442461490631104\n",
      "Epoch: 10 Step: 5325 Val Loss: 1.3475713729858398\n",
      "Epoch: 10 Step: 5350 Val Loss: 1.3397018909454346\n",
      "Epoch: 10 Step: 5375 Val Loss: 1.3428781032562256\n",
      "Epoch: 11 Step: 5400 Val Loss: 1.3390755653381348\n",
      "Epoch: 11 Step: 5425 Val Loss: 1.3340617418289185\n",
      "Epoch: 11 Step: 5450 Val Loss: 1.3411542177200317\n",
      "Epoch: 11 Step: 5475 Val Loss: 1.3335707187652588\n",
      "Epoch: 11 Step: 5500 Val Loss: 1.333674430847168\n",
      "Epoch: 11 Step: 5525 Val Loss: 1.332869529724121\n",
      "Epoch: 11 Step: 5550 Val Loss: 1.3339067697525024\n",
      "Epoch: 11 Step: 5575 Val Loss: 1.3313426971435547\n",
      "Epoch: 11 Step: 5600 Val Loss: 1.3344618082046509\n",
      "Epoch: 11 Step: 5625 Val Loss: 1.3335015773773193\n",
      "Epoch: 11 Step: 5650 Val Loss: 1.330646276473999\n",
      "Epoch: 11 Step: 5675 Val Loss: 1.3300855159759521\n",
      "Epoch: 11 Step: 5700 Val Loss: 1.3321049213409424\n",
      "Epoch: 11 Step: 5725 Val Loss: 1.3321722745895386\n",
      "Epoch: 11 Step: 5750 Val Loss: 1.3388885259628296\n",
      "Epoch: 11 Step: 5775 Val Loss: 1.3390676975250244\n",
      "Epoch: 11 Step: 5800 Val Loss: 1.340955138206482\n",
      "Epoch: 11 Step: 5825 Val Loss: 1.3384461402893066\n",
      "Epoch: 11 Step: 5850 Val Loss: 1.337833046913147\n",
      "Epoch: 11 Step: 5875 Val Loss: 1.3330053091049194\n",
      "Epoch: 12 Step: 5900 Val Loss: 1.3364657163619995\n",
      "Epoch: 12 Step: 5925 Val Loss: 1.333851933479309\n",
      "Epoch: 12 Step: 5950 Val Loss: 1.3363137245178223\n",
      "Epoch: 12 Step: 5975 Val Loss: 1.330142617225647\n",
      "Epoch: 12 Step: 6000 Val Loss: 1.3337582349777222\n",
      "Epoch: 12 Step: 6025 Val Loss: 1.3316149711608887\n",
      "Epoch: 12 Step: 6050 Val Loss: 1.3270041942596436\n",
      "Epoch: 12 Step: 6075 Val Loss: 1.329361081123352\n",
      "Epoch: 12 Step: 6100 Val Loss: 1.3376623392105103\n",
      "Epoch: 12 Step: 6125 Val Loss: 1.3301690816879272\n",
      "Epoch: 12 Step: 6150 Val Loss: 1.3278453350067139\n",
      "Epoch: 12 Step: 6175 Val Loss: 1.3260918855667114\n",
      "Epoch: 12 Step: 6200 Val Loss: 1.3291873931884766\n",
      "Epoch: 12 Step: 6225 Val Loss: 1.326420545578003\n",
      "Epoch: 12 Step: 6250 Val Loss: 1.332336664199829\n",
      "Epoch: 12 Step: 6275 Val Loss: 1.3291385173797607\n",
      "Epoch: 12 Step: 6300 Val Loss: 1.3328746557235718\n",
      "Epoch: 12 Step: 6325 Val Loss: 1.3286144733428955\n",
      "Epoch: 12 Step: 6350 Val Loss: 1.3334739208221436\n",
      "Epoch: 13 Step: 6375 Val Loss: 1.331845760345459\n",
      "Epoch: 13 Step: 6400 Val Loss: 1.3305282592773438\n",
      "Epoch: 13 Step: 6425 Val Loss: 1.3264983892440796\n",
      "Epoch: 13 Step: 6450 Val Loss: 1.3291022777557373\n",
      "Epoch: 13 Step: 6475 Val Loss: 1.3247642517089844\n",
      "Epoch: 13 Step: 6500 Val Loss: 1.32211172580719\n",
      "Epoch: 13 Step: 6525 Val Loss: 1.324925184249878\n",
      "Epoch: 13 Step: 6550 Val Loss: 1.3270360231399536\n",
      "Epoch: 13 Step: 6575 Val Loss: 1.3276787996292114\n",
      "Epoch: 13 Step: 6600 Val Loss: 1.3248822689056396\n",
      "Epoch: 13 Step: 6625 Val Loss: 1.3236998319625854\n",
      "Epoch: 13 Step: 6650 Val Loss: 1.3236992359161377\n",
      "Epoch: 13 Step: 6675 Val Loss: 1.32552170753479\n",
      "Epoch: 13 Step: 6700 Val Loss: 1.3223398923873901\n",
      "Epoch: 13 Step: 6725 Val Loss: 1.3285435438156128\n",
      "Epoch: 13 Step: 6750 Val Loss: 1.3252031803131104\n",
      "Epoch: 13 Step: 6775 Val Loss: 1.325668215751648\n",
      "Epoch: 13 Step: 6800 Val Loss: 1.3316519260406494\n",
      "Epoch: 13 Step: 6825 Val Loss: 1.3275068998336792\n",
      "Epoch: 13 Step: 6850 Val Loss: 1.3245469331741333\n",
      "Epoch: 14 Step: 6875 Val Loss: 1.320366382598877\n",
      "Epoch: 14 Step: 6900 Val Loss: 1.3199456930160522\n",
      "Epoch: 14 Step: 6925 Val Loss: 1.3178730010986328\n",
      "Epoch: 14 Step: 6950 Val Loss: 1.3200335502624512\n",
      "Epoch: 14 Step: 6975 Val Loss: 1.3203332424163818\n",
      "Epoch: 14 Step: 7000 Val Loss: 1.3190909624099731\n",
      "Epoch: 14 Step: 7025 Val Loss: 1.3248381614685059\n",
      "Epoch: 14 Step: 7050 Val Loss: 1.3200899362564087\n",
      "Epoch: 14 Step: 7075 Val Loss: 1.3211495876312256\n",
      "Epoch: 14 Step: 7100 Val Loss: 1.3195832967758179\n",
      "Epoch: 14 Step: 7125 Val Loss: 1.3108798265457153\n",
      "Epoch: 14 Step: 7150 Val Loss: 1.321854829788208\n",
      "Epoch: 14 Step: 7175 Val Loss: 1.3156627416610718\n",
      "Epoch: 14 Step: 7200 Val Loss: 1.319718837738037\n",
      "Epoch: 14 Step: 7225 Val Loss: 1.3256107568740845\n",
      "Epoch: 14 Step: 7250 Val Loss: 1.326265811920166\n",
      "Epoch: 14 Step: 7275 Val Loss: 1.3261433839797974\n",
      "Epoch: 14 Step: 7300 Val Loss: 1.3257100582122803\n",
      "Epoch: 14 Step: 7325 Val Loss: 1.3236340284347534\n",
      "Epoch: 14 Step: 7350 Val Loss: 1.3215094804763794\n",
      "Epoch: 15 Step: 7375 Val Loss: 1.3209983110427856\n",
      "Epoch: 15 Step: 7400 Val Loss: 1.323972463607788\n",
      "Epoch: 15 Step: 7425 Val Loss: 1.3206911087036133\n",
      "Epoch: 15 Step: 7450 Val Loss: 1.325390100479126\n",
      "Epoch: 15 Step: 7475 Val Loss: 1.3198858499526978\n",
      "Epoch: 15 Step: 7500 Val Loss: 1.3213350772857666\n",
      "Epoch: 15 Step: 7525 Val Loss: 1.3143278360366821\n",
      "Epoch: 15 Step: 7550 Val Loss: 1.3183826208114624\n",
      "Epoch: 15 Step: 7575 Val Loss: 1.3207768201828003\n",
      "Epoch: 15 Step: 7600 Val Loss: 1.3198603391647339\n",
      "Epoch: 15 Step: 7625 Val Loss: 1.3171387910842896\n",
      "Epoch: 15 Step: 7650 Val Loss: 1.3181031942367554\n",
      "Epoch: 15 Step: 7675 Val Loss: 1.3200807571411133\n",
      "Epoch: 15 Step: 7700 Val Loss: 1.326383113861084\n",
      "Epoch: 15 Step: 7725 Val Loss: 1.3233352899551392\n",
      "Epoch: 15 Step: 7750 Val Loss: 1.3259862661361694\n",
      "Epoch: 15 Step: 7775 Val Loss: 1.3204933404922485\n",
      "Epoch: 15 Step: 7800 Val Loss: 1.3214211463928223\n",
      "Epoch: 15 Step: 7825 Val Loss: 1.3207294940948486\n",
      "Epoch: 16 Step: 7850 Val Loss: 1.319940209388733\n",
      "Epoch: 16 Step: 7875 Val Loss: 1.3164825439453125\n",
      "Epoch: 16 Step: 7900 Val Loss: 1.323865532875061\n",
      "Epoch: 16 Step: 7925 Val Loss: 1.3223514556884766\n",
      "Epoch: 16 Step: 7950 Val Loss: 1.3226189613342285\n",
      "Epoch: 16 Step: 7975 Val Loss: 1.3173428773880005\n",
      "Epoch: 16 Step: 8000 Val Loss: 1.3192638158798218\n",
      "Epoch: 16 Step: 8025 Val Loss: 1.3179157972335815\n",
      "Epoch: 16 Step: 8050 Val Loss: 1.3141709566116333\n",
      "Epoch: 16 Step: 8075 Val Loss: 1.3176190853118896\n",
      "Epoch: 16 Step: 8100 Val Loss: 1.3159399032592773\n",
      "Epoch: 16 Step: 8125 Val Loss: 1.314597249031067\n",
      "Epoch: 16 Step: 8150 Val Loss: 1.3188780546188354\n",
      "Epoch: 16 Step: 8175 Val Loss: 1.316632866859436\n",
      "Epoch: 16 Step: 8200 Val Loss: 1.321736454963684\n",
      "Epoch: 16 Step: 8225 Val Loss: 1.3253889083862305\n",
      "Epoch: 16 Step: 8250 Val Loss: 1.3249553442001343\n",
      "Epoch: 16 Step: 8275 Val Loss: 1.3234816789627075\n",
      "Epoch: 16 Step: 8300 Val Loss: 1.3256646394729614\n",
      "Epoch: 16 Step: 8325 Val Loss: 1.3187342882156372\n",
      "Epoch: 17 Step: 8350 Val Loss: 1.3198566436767578\n",
      "Epoch: 17 Step: 8375 Val Loss: 1.3184814453125\n",
      "Epoch: 17 Step: 8400 Val Loss: 1.3231247663497925\n",
      "Epoch: 17 Step: 8425 Val Loss: 1.3140482902526855\n",
      "Epoch: 17 Step: 8450 Val Loss: 1.3170666694641113\n",
      "Epoch: 17 Step: 8475 Val Loss: 1.3158550262451172\n",
      "Epoch: 17 Step: 8500 Val Loss: 1.318222165107727\n",
      "Epoch: 17 Step: 8525 Val Loss: 1.3191535472869873\n",
      "Epoch: 17 Step: 8550 Val Loss: 1.319888710975647\n",
      "Epoch: 17 Step: 8575 Val Loss: 1.316685676574707\n",
      "Epoch: 17 Step: 8600 Val Loss: 1.3113826513290405\n",
      "Epoch: 17 Step: 8625 Val Loss: 1.3134483098983765\n",
      "Epoch: 17 Step: 8650 Val Loss: 1.3148630857467651\n",
      "Epoch: 17 Step: 8675 Val Loss: 1.313622236251831\n",
      "Epoch: 17 Step: 8700 Val Loss: 1.3142184019088745\n",
      "Epoch: 17 Step: 8725 Val Loss: 1.3181017637252808\n",
      "Epoch: 17 Step: 8750 Val Loss: 1.324367642402649\n",
      "Epoch: 17 Step: 8775 Val Loss: 1.319333791732788\n",
      "Epoch: 17 Step: 8800 Val Loss: 1.3163806200027466\n",
      "Epoch: 18 Step: 8825 Val Loss: 1.318110466003418\n",
      "Epoch: 18 Step: 8850 Val Loss: 1.3182293176651\n",
      "Epoch: 18 Step: 8875 Val Loss: 1.3195786476135254\n",
      "Epoch: 18 Step: 8900 Val Loss: 1.31980562210083\n",
      "Epoch: 18 Step: 8925 Val Loss: 1.3186219930648804\n",
      "Epoch: 18 Step: 8950 Val Loss: 1.3151651620864868\n",
      "Epoch: 18 Step: 8975 Val Loss: 1.3173329830169678\n",
      "Epoch: 18 Step: 9000 Val Loss: 1.3166913986206055\n",
      "Epoch: 18 Step: 9025 Val Loss: 1.3188633918762207\n",
      "Epoch: 18 Step: 9050 Val Loss: 1.3189592361450195\n",
      "Epoch: 18 Step: 9075 Val Loss: 1.309308648109436\n",
      "Epoch: 18 Step: 9100 Val Loss: 1.3131492137908936\n",
      "Epoch: 18 Step: 9125 Val Loss: 1.3100746870040894\n",
      "Epoch: 18 Step: 9150 Val Loss: 1.3130085468292236\n",
      "Epoch: 18 Step: 9175 Val Loss: 1.3179340362548828\n",
      "Epoch: 18 Step: 9200 Val Loss: 1.3193413019180298\n",
      "Epoch: 18 Step: 9225 Val Loss: 1.3207521438598633\n",
      "Epoch: 18 Step: 9250 Val Loss: 1.3226985931396484\n",
      "Epoch: 18 Step: 9275 Val Loss: 1.3143346309661865\n",
      "Epoch: 18 Step: 9300 Val Loss: 1.3152762651443481\n",
      "Epoch: 19 Step: 9325 Val Loss: 1.312779426574707\n",
      "Epoch: 19 Step: 9350 Val Loss: 1.3123221397399902\n",
      "Epoch: 19 Step: 9375 Val Loss: 1.31779146194458\n",
      "Epoch: 19 Step: 9400 Val Loss: 1.3140332698822021\n",
      "Epoch: 19 Step: 9425 Val Loss: 1.3165556192398071\n",
      "Epoch: 19 Step: 9450 Val Loss: 1.3110538721084595\n",
      "Epoch: 19 Step: 9475 Val Loss: 1.3115352392196655\n",
      "Epoch: 19 Step: 9500 Val Loss: 1.31173574924469\n",
      "Epoch: 19 Step: 9525 Val Loss: 1.310426115989685\n",
      "Epoch: 19 Step: 9550 Val Loss: 1.3107472658157349\n",
      "Epoch: 19 Step: 9575 Val Loss: 1.3086885213851929\n",
      "Epoch: 19 Step: 9600 Val Loss: 1.313109278678894\n",
      "Epoch: 19 Step: 9625 Val Loss: 1.3081244230270386\n",
      "Epoch: 19 Step: 9650 Val Loss: 1.3136638402938843\n",
      "Epoch: 19 Step: 9675 Val Loss: 1.3210219144821167\n",
      "Epoch: 19 Step: 9700 Val Loss: 1.3190046548843384\n",
      "Epoch: 19 Step: 9725 Val Loss: 1.31717050075531\n",
      "Epoch: 19 Step: 9750 Val Loss: 1.3192051649093628\n",
      "Epoch: 19 Step: 9775 Val Loss: 1.3178672790527344\n",
      "Epoch: 19 Step: 9800 Val Loss: 1.3181393146514893\n",
      "Epoch: 20 Step: 9825 Val Loss: 1.3133187294006348\n",
      "Epoch: 20 Step: 9850 Val Loss: 1.3153982162475586\n",
      "Epoch: 20 Step: 9875 Val Loss: 1.3176625967025757\n",
      "Epoch: 20 Step: 9900 Val Loss: 1.3155463933944702\n",
      "Epoch: 20 Step: 9925 Val Loss: 1.3123393058776855\n",
      "Epoch: 20 Step: 9950 Val Loss: 1.3082305192947388\n",
      "Epoch: 20 Step: 9975 Val Loss: 1.3077449798583984\n",
      "Epoch: 20 Step: 10000 Val Loss: 1.3116283416748047\n",
      "Epoch: 20 Step: 10025 Val Loss: 1.3156465291976929\n",
      "Epoch: 20 Step: 10050 Val Loss: 1.3133764266967773\n",
      "Epoch: 20 Step: 10075 Val Loss: 1.3109476566314697\n",
      "Epoch: 20 Step: 10100 Val Loss: 1.311468243598938\n",
      "Epoch: 20 Step: 10125 Val Loss: 1.3087685108184814\n",
      "Epoch: 20 Step: 10150 Val Loss: 1.316538691520691\n",
      "Epoch: 20 Step: 10175 Val Loss: 1.3133069276809692\n",
      "Epoch: 20 Step: 10200 Val Loss: 1.3076130151748657\n",
      "Epoch: 20 Step: 10225 Val Loss: 1.3133924007415771\n",
      "Epoch: 20 Step: 10250 Val Loss: 1.3132104873657227\n",
      "Epoch: 20 Step: 10275 Val Loss: 1.312880039215088\n",
      "Epoch: 21 Step: 10300 Val Loss: 1.3107502460479736\n",
      "Epoch: 21 Step: 10325 Val Loss: 1.3046436309814453\n",
      "Epoch: 21 Step: 10350 Val Loss: 1.3121802806854248\n",
      "Epoch: 21 Step: 10375 Val Loss: 1.3075653314590454\n",
      "Epoch: 21 Step: 10400 Val Loss: 1.3047200441360474\n",
      "Epoch: 21 Step: 10425 Val Loss: 1.3046954870224\n",
      "Epoch: 21 Step: 10450 Val Loss: 1.309462070465088\n",
      "Epoch: 21 Step: 10475 Val Loss: 1.3105844259262085\n",
      "Epoch: 21 Step: 10500 Val Loss: 1.3065521717071533\n",
      "Epoch: 21 Step: 10525 Val Loss: 1.3080404996871948\n",
      "Epoch: 21 Step: 10550 Val Loss: 1.3045662641525269\n",
      "Epoch: 21 Step: 10575 Val Loss: 1.3100794553756714\n",
      "Epoch: 21 Step: 10600 Val Loss: 1.3109593391418457\n",
      "Epoch: 21 Step: 10625 Val Loss: 1.3076562881469727\n",
      "Epoch: 21 Step: 10650 Val Loss: 1.3114739656448364\n",
      "Epoch: 21 Step: 10675 Val Loss: 1.3124316930770874\n",
      "Epoch: 21 Step: 10700 Val Loss: 1.3164966106414795\n",
      "Epoch: 21 Step: 10725 Val Loss: 1.3171000480651855\n",
      "Epoch: 21 Step: 10750 Val Loss: 1.3144620656967163\n",
      "Epoch: 21 Step: 10775 Val Loss: 1.3125735521316528\n",
      "Epoch: 22 Step: 10800 Val Loss: 1.3122074604034424\n",
      "Epoch: 22 Step: 10825 Val Loss: 1.30887770652771\n",
      "Epoch: 22 Step: 10850 Val Loss: 1.3164411783218384\n",
      "Epoch: 22 Step: 10875 Val Loss: 1.310754656791687\n",
      "Epoch: 22 Step: 10900 Val Loss: 1.313040018081665\n",
      "Epoch: 22 Step: 10925 Val Loss: 1.3083744049072266\n",
      "Epoch: 22 Step: 10950 Val Loss: 1.3074859380722046\n",
      "Epoch: 22 Step: 10975 Val Loss: 1.30611252784729\n",
      "Epoch: 22 Step: 11000 Val Loss: 1.3087012767791748\n",
      "Epoch: 22 Step: 11025 Val Loss: 1.3068771362304688\n",
      "Epoch: 22 Step: 11050 Val Loss: 1.3054568767547607\n",
      "Epoch: 22 Step: 11075 Val Loss: 1.3089148998260498\n",
      "Epoch: 22 Step: 11100 Val Loss: 1.3092427253723145\n",
      "Epoch: 22 Step: 11125 Val Loss: 1.3099697828292847\n",
      "Epoch: 22 Step: 11150 Val Loss: 1.307880163192749\n",
      "Epoch: 22 Step: 11175 Val Loss: 1.3132905960083008\n",
      "Epoch: 22 Step: 11200 Val Loss: 1.312464952468872\n",
      "Epoch: 22 Step: 11225 Val Loss: 1.3113317489624023\n",
      "Epoch: 22 Step: 11250 Val Loss: 1.309136986732483\n",
      "Epoch: 23 Step: 11275 Val Loss: 1.3131614923477173\n",
      "Epoch: 23 Step: 11300 Val Loss: 1.3086966276168823\n",
      "Epoch: 23 Step: 11325 Val Loss: 1.312721848487854\n",
      "Epoch: 23 Step: 11350 Val Loss: 1.310623288154602\n",
      "Epoch: 23 Step: 11375 Val Loss: 1.3149491548538208\n",
      "Epoch: 23 Step: 11400 Val Loss: 1.3082826137542725\n",
      "Epoch: 23 Step: 11425 Val Loss: 1.3081268072128296\n",
      "Epoch: 23 Step: 11450 Val Loss: 1.307935118675232\n",
      "Epoch: 23 Step: 11475 Val Loss: 1.3099448680877686\n",
      "Epoch: 23 Step: 11500 Val Loss: 1.3087756633758545\n",
      "Epoch: 23 Step: 11525 Val Loss: 1.3053807020187378\n",
      "Epoch: 23 Step: 11550 Val Loss: 1.3095449209213257\n",
      "Epoch: 23 Step: 11575 Val Loss: 1.303274393081665\n",
      "Epoch: 23 Step: 11600 Val Loss: 1.3076430559158325\n",
      "Epoch: 23 Step: 11625 Val Loss: 1.313391089439392\n",
      "Epoch: 23 Step: 11650 Val Loss: 1.3120976686477661\n",
      "Epoch: 23 Step: 11675 Val Loss: 1.3122144937515259\n",
      "Epoch: 23 Step: 11700 Val Loss: 1.3163394927978516\n",
      "Epoch: 23 Step: 11725 Val Loss: 1.3120219707489014\n",
      "Epoch: 23 Step: 11750 Val Loss: 1.3107340335845947\n",
      "Epoch: 24 Step: 11775 Val Loss: 1.3112337589263916\n",
      "Epoch: 24 Step: 11800 Val Loss: 1.3107784986495972\n",
      "Epoch: 24 Step: 11825 Val Loss: 1.31016206741333\n",
      "Epoch: 24 Step: 11850 Val Loss: 1.309752345085144\n",
      "Epoch: 24 Step: 11875 Val Loss: 1.3104599714279175\n",
      "Epoch: 24 Step: 11900 Val Loss: 1.3063275814056396\n",
      "Epoch: 24 Step: 11925 Val Loss: 1.3050518035888672\n",
      "Epoch: 24 Step: 11950 Val Loss: 1.3104153871536255\n",
      "Epoch: 24 Step: 11975 Val Loss: 1.3096452951431274\n",
      "Epoch: 24 Step: 12000 Val Loss: 1.3087726831436157\n",
      "Epoch: 24 Step: 12025 Val Loss: 1.3082228899002075\n",
      "Epoch: 24 Step: 12050 Val Loss: 1.3080118894577026\n",
      "Epoch: 24 Step: 12075 Val Loss: 1.3052297830581665\n",
      "Epoch: 24 Step: 12100 Val Loss: 1.310068964958191\n",
      "Epoch: 24 Step: 12125 Val Loss: 1.3125766515731812\n",
      "Epoch: 24 Step: 12150 Val Loss: 1.3165838718414307\n",
      "Epoch: 24 Step: 12175 Val Loss: 1.317824125289917\n",
      "Epoch: 24 Step: 12200 Val Loss: 1.3127540349960327\n",
      "Epoch: 24 Step: 12225 Val Loss: 1.3148607015609741\n",
      "Epoch: 24 Step: 12250 Val Loss: 1.311537742614746\n",
      "Epoch: 25 Step: 12275 Val Loss: 1.3137140274047852\n",
      "Epoch: 25 Step: 12300 Val Loss: 1.3103517293930054\n",
      "Epoch: 25 Step: 12325 Val Loss: 1.3139679431915283\n",
      "Epoch: 25 Step: 12350 Val Loss: 1.3152703046798706\n",
      "Epoch: 25 Step: 12375 Val Loss: 1.310112714767456\n",
      "Epoch: 25 Step: 12400 Val Loss: 1.3114466667175293\n",
      "Epoch: 25 Step: 12425 Val Loss: 1.30501127243042\n",
      "Epoch: 25 Step: 12450 Val Loss: 1.310665488243103\n",
      "Epoch: 25 Step: 12475 Val Loss: 1.3115246295928955\n",
      "Epoch: 25 Step: 12500 Val Loss: 1.3106011152267456\n",
      "Epoch: 25 Step: 12525 Val Loss: 1.306729793548584\n",
      "Epoch: 25 Step: 12550 Val Loss: 1.3025928735733032\n",
      "Epoch: 25 Step: 12575 Val Loss: 1.3053748607635498\n",
      "Epoch: 25 Step: 12600 Val Loss: 1.3110272884368896\n",
      "Epoch: 25 Step: 12625 Val Loss: 1.307097315788269\n",
      "Epoch: 25 Step: 12650 Val Loss: 1.3112987279891968\n",
      "Epoch: 25 Step: 12675 Val Loss: 1.3109605312347412\n",
      "Epoch: 25 Step: 12700 Val Loss: 1.3076413869857788\n",
      "Epoch: 25 Step: 12725 Val Loss: 1.3088457584381104\n",
      "Epoch: 26 Step: 12750 Val Loss: 1.306738257408142\n",
      "Epoch: 26 Step: 12775 Val Loss: 1.3035759925842285\n",
      "Epoch: 26 Step: 12800 Val Loss: 1.3073731660842896\n",
      "Epoch: 26 Step: 12825 Val Loss: 1.3066157102584839\n",
      "Epoch: 26 Step: 12850 Val Loss: 1.3097530603408813\n",
      "Epoch: 26 Step: 12875 Val Loss: 1.3022387027740479\n",
      "Epoch: 26 Step: 12900 Val Loss: 1.3089184761047363\n",
      "Epoch: 26 Step: 12925 Val Loss: 1.308854579925537\n",
      "Epoch: 26 Step: 12950 Val Loss: 1.3037803173065186\n",
      "Epoch: 26 Step: 12975 Val Loss: 1.303592324256897\n",
      "Epoch: 26 Step: 13000 Val Loss: 1.3024147748947144\n",
      "Epoch: 26 Step: 13025 Val Loss: 1.3058195114135742\n",
      "Epoch: 26 Step: 13050 Val Loss: 1.3006770610809326\n",
      "Epoch: 26 Step: 13075 Val Loss: 1.3018243312835693\n",
      "Epoch: 26 Step: 13100 Val Loss: 1.3075357675552368\n",
      "Epoch: 26 Step: 13125 Val Loss: 1.310927391052246\n",
      "Epoch: 26 Step: 13150 Val Loss: 1.3123435974121094\n",
      "Epoch: 26 Step: 13175 Val Loss: 1.3114219903945923\n",
      "Epoch: 26 Step: 13200 Val Loss: 1.3077012300491333\n",
      "Epoch: 26 Step: 13225 Val Loss: 1.3095099925994873\n",
      "Epoch: 27 Step: 13250 Val Loss: 1.309096097946167\n",
      "Epoch: 27 Step: 13275 Val Loss: 1.3084200620651245\n",
      "Epoch: 27 Step: 13300 Val Loss: 1.311071753501892\n",
      "Epoch: 27 Step: 13325 Val Loss: 1.308436393737793\n",
      "Epoch: 27 Step: 13350 Val Loss: 1.3071480989456177\n",
      "Epoch: 27 Step: 13375 Val Loss: 1.3065576553344727\n",
      "Epoch: 27 Step: 13400 Val Loss: 1.3039391040802002\n",
      "Epoch: 27 Step: 13425 Val Loss: 1.3064820766448975\n",
      "Epoch: 27 Step: 13450 Val Loss: 1.3091131448745728\n",
      "Epoch: 27 Step: 13475 Val Loss: 1.3086377382278442\n",
      "Epoch: 27 Step: 13500 Val Loss: 1.301607608795166\n",
      "Epoch: 27 Step: 13525 Val Loss: 1.300842523574829\n",
      "Epoch: 27 Step: 13550 Val Loss: 1.2950228452682495\n",
      "Epoch: 27 Step: 13575 Val Loss: 1.3050416707992554\n",
      "Epoch: 27 Step: 13600 Val Loss: 1.3082687854766846\n",
      "Epoch: 27 Step: 13625 Val Loss: 1.309571623802185\n",
      "Epoch: 27 Step: 13650 Val Loss: 1.3114932775497437\n",
      "Epoch: 27 Step: 13675 Val Loss: 1.3092126846313477\n",
      "Epoch: 27 Step: 13700 Val Loss: 1.3087048530578613\n",
      "Epoch: 28 Step: 13725 Val Loss: 1.3071041107177734\n",
      "Epoch: 28 Step: 13750 Val Loss: 1.3086931705474854\n",
      "Epoch: 28 Step: 13775 Val Loss: 1.3089840412139893\n",
      "Epoch: 28 Step: 13800 Val Loss: 1.304833173751831\n",
      "Epoch: 28 Step: 13825 Val Loss: 1.3054927587509155\n",
      "Epoch: 28 Step: 13850 Val Loss: 1.2983394861221313\n",
      "Epoch: 28 Step: 13875 Val Loss: 1.3024076223373413\n",
      "Epoch: 28 Step: 13900 Val Loss: 1.303560495376587\n",
      "Epoch: 28 Step: 13925 Val Loss: 1.3008166551589966\n",
      "Epoch: 28 Step: 13950 Val Loss: 1.2982226610183716\n",
      "Epoch: 28 Step: 13975 Val Loss: 1.302610158920288\n",
      "Epoch: 28 Step: 14000 Val Loss: 1.302781343460083\n",
      "Epoch: 28 Step: 14025 Val Loss: 1.3013734817504883\n",
      "Epoch: 28 Step: 14050 Val Loss: 1.3057403564453125\n",
      "Epoch: 28 Step: 14075 Val Loss: 1.3074127435684204\n",
      "Epoch: 28 Step: 14100 Val Loss: 1.3028401136398315\n",
      "Epoch: 28 Step: 14125 Val Loss: 1.3026574850082397\n",
      "Epoch: 28 Step: 14150 Val Loss: 1.3127304315567017\n",
      "Epoch: 28 Step: 14175 Val Loss: 1.3099297285079956\n",
      "Epoch: 28 Step: 14200 Val Loss: 1.3089861869812012\n",
      "Epoch: 29 Step: 14225 Val Loss: 1.3085097074508667\n",
      "Epoch: 29 Step: 14250 Val Loss: 1.301985740661621\n",
      "Epoch: 29 Step: 14275 Val Loss: 1.3051260709762573\n",
      "Epoch: 29 Step: 14300 Val Loss: 1.3080427646636963\n",
      "Epoch: 29 Step: 14325 Val Loss: 1.3103914260864258\n",
      "Epoch: 29 Step: 14350 Val Loss: 1.3054893016815186\n",
      "Epoch: 29 Step: 14375 Val Loss: 1.3048492670059204\n",
      "Epoch: 29 Step: 14400 Val Loss: 1.3091282844543457\n",
      "Epoch: 29 Step: 14425 Val Loss: 1.3064939975738525\n",
      "Epoch: 29 Step: 14450 Val Loss: 1.3048142194747925\n",
      "Epoch: 29 Step: 14475 Val Loss: 1.304341197013855\n",
      "Epoch: 29 Step: 14500 Val Loss: 1.307332158088684\n",
      "Epoch: 29 Step: 14525 Val Loss: 1.301115870475769\n",
      "Epoch: 29 Step: 14550 Val Loss: 1.301946759223938\n",
      "Epoch: 29 Step: 14575 Val Loss: 1.3099628686904907\n",
      "Epoch: 29 Step: 14600 Val Loss: 1.312131404876709\n",
      "Epoch: 29 Step: 14625 Val Loss: 1.3082538843154907\n",
      "Epoch: 29 Step: 14650 Val Loss: 1.3089510202407837\n",
      "Epoch: 29 Step: 14675 Val Loss: 1.3075101375579834\n",
      "Epoch: 29 Step: 14700 Val Loss: 1.3091001510620117\n"
     ]
    }
   ],
   "source": [
    "# train the model!\n",
    "# Set model to train\n",
    "model.train()\n",
    "\n",
    "\n",
    "# Check to see if using GPU\n",
    "if model.use_gpu:\n",
    "    model.cuda()\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    hidden = model.hidden_state(batch_size)\n",
    "    \n",
    "    \n",
    "    for x,y in generate_batches(train_data,batch_size,seq_len):\n",
    "        \n",
    "        tracker += 1\n",
    "        \n",
    "        # One Hot Encode incoming data\n",
    "        x = one_hot_encoder(x,num_char)\n",
    "        \n",
    "        # Convert Numpy Arrays to Tensor\n",
    "        \n",
    "        inputs = torch.from_numpy(x)\n",
    "        targets = torch.from_numpy(y)\n",
    "        \n",
    "        # Adjust for GPU if necessary\n",
    "        \n",
    "        if model.use_gpu:\n",
    "            \n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "            \n",
    "        # Reset Hidden State\n",
    "        # If we dont' reset we would backpropagate through all training history\n",
    "        hidden = tuple([state.data for state in hidden])\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        lstm_output, hidden = model.forward(inputs,hidden)\n",
    "        loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # POSSIBLE EXPLODING GRADIENT PROBLEM!\n",
    "        # LET\"S CLIP JUST IN CASE\n",
    "        nn.utils.clip_grad_norm_(model.parameters(),max_norm=5)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        ###################################\n",
    "        ### CHECK ON VALIDATION SET ######\n",
    "        #################################\n",
    "        \n",
    "        if tracker % 25 == 0:\n",
    "            \n",
    "            val_hidden = model.hidden_state(batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            \n",
    "            for x,y in generate_batches(val_data,batch_size,seq_len):\n",
    "                \n",
    "                # One Hot Encode incoming data\n",
    "                x = one_hot_encoder(x,num_char)\n",
    "                \n",
    "\n",
    "                # Convert Numpy Arrays to Tensor\n",
    "\n",
    "                inputs = torch.from_numpy(x)\n",
    "                targets = torch.from_numpy(y)\n",
    "\n",
    "                # Adjust for GPU if necessary\n",
    "\n",
    "                if model.use_gpu:\n",
    "\n",
    "                    inputs = inputs.cuda()\n",
    "                    targets = targets.cuda()\n",
    "                    \n",
    "                # Reset Hidden State\n",
    "                # If we dont' reset we would backpropagate through \n",
    "                # all training history\n",
    "                val_hidden = tuple([state.data for state in val_hidden])\n",
    "                \n",
    "                lstm_output, val_hidden = model.forward(inputs,val_hidden)\n",
    "                val_loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())\n",
    "        \n",
    "                val_losses.append(val_loss.item())\n",
    "            \n",
    "            # Reset to training model after val for loop\n",
    "            model.train()\n",
    "            \n",
    "            print(f\"Epoch: {i} Step: {tracker} Val Loss: {val_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model\n",
    "model_name = 'example_hidden512_layers3_sonnet.net'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model, must match exact settings as model used during training!\n",
    "model = CharModel(\n",
    "    all_chars=all_characters,\n",
    "    num_hidden=512,\n",
    "    num_layers=3,\n",
    "    drop_prob=0.5,\n",
    "    use_gpu=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(model_name))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating predicted text\n",
    "def predict_next_char(model, char, hidden=None, k=1):\n",
    "        \n",
    "        # Encode raw letters with model\n",
    "        encoded_text = model.encoder[char]\n",
    "        \n",
    "        # set as numpy array for one hot encoding\n",
    "        # NOTE THE [[ ]] dimensions!!\n",
    "        encoded_text = np.array([[encoded_text]])\n",
    "        \n",
    "        # One hot encoding\n",
    "        encoded_text = one_hot_encoder(encoded_text, len(model.all_chars))\n",
    "        \n",
    "        # Convert to Tensor\n",
    "        inputs = torch.from_numpy(encoded_text)\n",
    "        \n",
    "        # Check for CPU\n",
    "        if(model.use_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        \n",
    "        # Grab hidden states\n",
    "        hidden = tuple([state.data for state in hidden])\n",
    "        \n",
    "        \n",
    "        # Run model and get predicted output\n",
    "        lstm_out, hidden = model(inputs, hidden)\n",
    "\n",
    "        \n",
    "        # Convert lstm_out to probabilities\n",
    "        probs = F.softmax(lstm_out, dim=1).data\n",
    "        \n",
    "        \n",
    "        \n",
    "        if(model.use_gpu):\n",
    "            # move back to CPU to use with numpy\n",
    "            probs = probs.cpu()\n",
    "        \n",
    "        \n",
    "        # k determines how many characters to consider\n",
    "        # for our probability choice.\n",
    "        # https://pytorch.org/docs/stable/torch.html#torch.topk\n",
    "        \n",
    "        # Return k largest probabilities in tensor\n",
    "        probs, index_positions = probs.topk(k)\n",
    "        \n",
    "        \n",
    "        index_positions = index_positions.numpy().squeeze()\n",
    "        \n",
    "        # Create array of probabilities\n",
    "        probs = probs.numpy().flatten()\n",
    "        \n",
    "        # Convert to probabilities per index\n",
    "        probs = probs/probs.sum()\n",
    "        \n",
    "        # randomly choose a character based on probabilities\n",
    "        char = np.random.choice(index_positions, p=probs)\n",
    "       \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return model.decoder[char], hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, size, seed='The', k=1):\n",
    "        \n",
    "      \n",
    "    \n",
    "    # CHECK FOR GPU\n",
    "    if(model.use_gpu):\n",
    "        model.cuda()\n",
    "    else:\n",
    "        model.cpu()\n",
    "    \n",
    "    # Evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # begin output from initial seed\n",
    "    output_chars = [c for c in seed]\n",
    "    \n",
    "    # intiate hidden state\n",
    "    hidden = model.hidden_state(1)\n",
    "    \n",
    "    # predict the next character for every character in seed\n",
    "    for char in seed:\n",
    "        char, hidden = predict_next_char(model, char, hidden, k=k)\n",
    "    \n",
    "    # add initial characters to output\n",
    "    output_chars.append(char)\n",
    "    \n",
    "    # Now generate for size requested\n",
    "    for i in range(size):\n",
    "        \n",
    "        # predict based off very last letter in output_chars\n",
    "        char, hidden = predict_next_char(model, output_chars[-1], hidden, k=k)\n",
    "        \n",
    "        # add predicted character\n",
    "        output_chars.append(char)\n",
    "    \n",
    "    # return string of predicted text\n",
    "    return ''.join(output_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The walls with the stage\n",
      "\n",
      "                             Enter CAIUS\n",
      "\n",
      "    This stands of the standing stand as she,\n",
      "    As they are sent to make my soul that will\n",
      "    As well defend the cheeks.\n",
      "  CORIOLANUS. I have a man of him and make me.\n",
      "  CLOWN. I am sure you are not, sir, which, in the face\n",
      "    And state the mother of my strange things,\n",
      "    And so much to your senses. I am sorry,\n",
      "    And we assure the state to thee again.\n",
      "    What should I see you then that I did see\n",
      "    The man of my soul and my stomach to the stock\n",
      "    When I am sure, as I have said, the maid\n",
      "    Was not a man of honesty than those\n",
      "    Will stranger so so much of me to sent you.  \n",
      "    This shall not take mine own desire of him.\n",
      "  KING RICHARD. I will not hear thee see the story of him,\n",
      "    Will I not see thy self-a father's soldiers;\n",
      "    And that the fool, the sea, with his dead soldiers\n",
      "    And stand to soldiers, and his strokes to straight\n",
      "    As towns are sense and fathers of their father.\n",
      "    And so I should not be\n"
     ]
    }
   ],
   "source": [
    "# write a sonnet with a starting seed word\n",
    "print(generate_text(model, 1000, seed='The ', k=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypytorchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
